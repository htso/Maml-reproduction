"""
Usage Instructions:
    Harmonic functions (K=100) :
        python main_fun.py --datasource=fun --metatrain_iterations=100000 --meta_batch_size=10 --update_batch_size=100 --train=True
        python main_fun.py --datasource=fun --metatrain_iterations=100000 --meta_batch_size=10 --update_batch_size=100 --train=False

    10-shot sinusoid:
        python main_fun.py --datasource=sinusoid --logdir=logs/sine --metatrain_iterations=70000 --update_batch_size=10

    Test on trained model:    
        python main_fun.py --datasource=sinusoid --logdir=logs/sine --metatrain_iterations=70000 --update_batch_size=10 --train=False

    10-shot sinusoid baselines:
        python main_fun.py --datasource=sinusoid --logdir=logs/sine --pretrain_iterations=70000 --metatrain_iterations=0 --update_batch_size=10 --baseline=oracle
        python main_fun.py --datasource=sinusoid --logdir=logs/sine --pretrain_iterations=70000 --metatrain_iterations=0 --update_batch_size=10

    To run evaluation, use the '--train=False' flag and the '--test_set=True' flag to use the test set.


========== EXPLAINING MAML METHODOLOGY AND CODE (TO MYSELF) ===============================================================================

    Finn's Variable Names & Meta Learning Concepts

    meaning of 'batch' : one batch is one sets of sine functions; each row in a batch represents a *different* function characterized by 
        a different amplitude and phase. Each batch is obtained by calling the generator. 
        Thus, each call of the generator produces one set of functions.

    update_batch_size/train_update_batch_size : number of data points on the function to train a FC model. In the standard
        ML framework, this is the total number of data points a model would see to learn its structure. This is the K in
        K-shot learning. Finn's usage is to have K data points for training, and equal number of points for validation.

    meta_batch_size : number of functions in a batch, which she calls "meta batch".

    num_samples_per_class : number of data points per function

    metatrain_iterations : number of times the generator is called. This multiplied by batch_size gives the total number of 
            functions the algorithm has seen over the entire course of training, assuming pretrain_iterations=0.

                # functions ever examined = (metatrain_iterations + pretrain_iterations) * meta_batch_size

    num_updates : number of gradient updates to make for each model training to learn a specific sine function.

    update_lr : the learning rate for individual model learning a sine function. Same for all models. This is \alpha
            in Finn's paper.

    meta_lr : learning rate to update the initial weights. This is \beta in Eq (1). 

    inputa, inputb : for each dataset of a function, 'a' is the training set, 'b' is the validation set. 
        These two objects are tensor of shape (meta_batch_size, 2*K, 1). Each row is the data points from one function.


    
    MAKING SENSE OF THE CODE

    Two sets of data are used to train a meta-model. They are called inputa, inputb, which appear inside the iter loop in the train function.
    A batch of data is generated by the data_generator, which returns a (x,y) pair. They are splitted into two halves. 
    The first K data points are the inputa, while the rest is assigned to inputb, where K is 'update_batch_size'. Thus, K is the
    size of the training set, and also the size of the valiation set. These two variables have shape,
          
            inputa, inputb = (batch size, 2*K, 1) 

    where batch size is the 'meta_batch_size', ie. the number of datasets. For the sinusoid generator, 2*K is the number of points 
    that a function is evaluated on. Since these 2*K points are generated at random, they are not ordered. That is, a point x_2 is 
    not necessarily on the right of x_1.

    In task_metalearn, inputa is used to compute a loss (task_lossesa) and then generate a gradient for each dataset, or each batch. 
    Then the model parameter theta, or 'weights' is updated with one or more gradient descent step. A second loss (task_lossesb) 
    is calculated using inputb. Keep in mind that both 'inputa' and 'inputb' come from the *same* sinusoid function. 
    They only differ in the location where the sine function is evaluated. 

    The gradient is calculated off inputa. As a result, the weight update is also based on inputa. However, the loss on inputb is 
    used to optimize the model parameter theta, which is the initial weight from which the individual neural network begins
    training.

    The codes outside of task_metalearn in MAML class definition need some explanation. First, a word on variable relationship. 
    lossesa and lossesb, each a list, are the loss computed over the entire meta batch, associated with inputa and inputb. 
    total_loss1 is a single value which sums up the individual loss of the batch. total_loss2 is a list, where each element 
    is the batch total of the losses at each gradient update. Thus, total_loss2 has length equal to number of gradient update steps. 

    When optimization is performed, it is performed with respect to the *last* value of total_loss2, as could be seen in this line 

            self.gvs = gvs = optimizer.compute_gradients(self.total_losses2[FLAGS.num_updates-1])
    
    where num_updates is the number of gradient updates carried out in task_metalearn. This makes sense because all you care is 
    the final theta after as many gradient steps. 
    
    In each iteration in train(), one batch of datasets are drawn, each dataset is a sine function with a particular amplitude
    and phase. This batch is used to optimize theta ('weights'), which is the starting weight of the individual neural network
    for each dataset. The overall meta model is optimized so that on average, each individual model would deliver the best 
    fit on an out-of-sample data (inputb) starting from the initial weight theta. 

    Ref : train() in main.py, task_metalearn() in class MAML in maml.py. 

    RESULTS ON FUNCTIONAL SHAPES

    I used MAML to learn some functional shapes other than the phase-shifting sinusoid. Instead of a sine wave with fixed frequency,
    a product of sine and cosine is returned from the data generator, where each draw takes a shape in which the sine and 
    cosine phase randomly changes. The frequency of the sine and cosine is fixed at 1.0 and 3.0; this is to make it easier
    to learn the harmonics. Each batch of "tasks", or datasets from the generator consists of different functional shapes 
    with x lies between 0.0 and 3.0. The first half of points are used as 'inputa' to produce the gradients, while the second
    half is the 'inputb' whose goodness of fit is used to update the meta-learning parameter theta. At each meta-learning iteration,
    the generator is set (Train=False) so that the data points are in chronological order, ie. the first point is the farthest 
    to the left with x=0.0, and the last the farthest to the right where x=3.0. Each element in a batch is a random shape with 
    parameters (the two phases) drawn from the grid matrix. A gaussian noise with a small variance is added to function. I've
    experimented without the random noise and the results are similar. The data generator is coded in FunGenerator.py.

    To make it easier for MAML to learn the oscillatory behavior of these funcitons, I increased K (update_batch_size) substantially.
    Instead of 10 shots, I used 100. The batch size (meta_batch_size) is set to 10, which is a rather arbitrary decision, but I 
    think it doesn't matter much. I ran 100k iterations; this is probably more than sufficient for any possible convergence to occur.

    The trained meta-learning model is tested on datasets drawn from the same generator with the same parameters. In other orders,
    this is just testing on the same universe of tasks as in training. The test setting uses K=10, but increasing K did not get
    any better. The result is plotted in 'FunShapes_MAML_model_Fit_Predict_Plots-100steps.pdf'.

    The main observation is MAML could not learn these slightly more complex functions. Focusing on the right plots, it's 
    obvious that the predicted variation doesn't come close to the periodicity of the true function. The local peaks of
    predict do not coincide with the cosine subwave. The second observation is that more gradient updates do not help
    improve the fit; comparing the one-step fit vs the 10-step fit, there is hardly any difference. In fact, looking at the
    mean losses, they actually increase as more gradient steps are processed. This is very strange. The prediction
    seems to be identical for all five functions; observe that the red and green lines are the same in all five plots on
    the right.

    This suggests MAML has very limited learning capability on regression problems. 

===========================================================================================================================================    
"""
import sys
import os
import csv
import numpy as np
import pickle
import random
import matplotlib.pyplot as plt
import tensorflow as tf
from data_generator_ht import DataGenerator
from FunGenerator import FunGenerator
from maml_ht import MAML
from tensorflow.python.platform import flags
from matplotlib.backends.backend_pdf import PdfPages

FLAGS = flags.FLAGS

## Dataset/method options
flags.DEFINE_string('datasource', 'fun', 'sinusoid or fun')
## Training options
flags.DEFINE_integer('update_batch_size', 100, 'number of examples used for inner gradient update (the K in K-shot learning).')
flags.DEFINE_integer('metatrain_iterations', 10000, 'number of metatraining iterations.') # 15k for omniglot, 50k for sinusoid
flags.DEFINE_integer('meta_batch_size', 32, 'number of tasks sampled per meta-update')
flags.DEFINE_integer('pretrain_iterations', 0, 'number of pre-training iterations.') # default is no pretraining
flags.DEFINE_float('meta_lr', 0.001, 'the base learning rate of the generator')
flags.DEFINE_float('update_lr', 1e-3, 'step size alpha for inner gradient update.') # 0.1 for omniglot
flags.DEFINE_integer('num_updates', 100, 'number of inner gradient updates during training.')

# oracle means task id is input (only suitable for sinusoid)
flags.DEFINE_string('baseline', None, 'oracle, or None')

## Model options
flags.DEFINE_string('norm', 'batch_norm', 'batch_norm, layer_norm, or None')
#flags.DEFINE_integer('num_filters', 64, 'number of filters for conv nets -- 32 for miniimagenet, 64 for omiglot.')
#flags.DEFINE_bool('conv', True, 'whether or not to use a convolutional network, only applicable in some cases')
#flags.DEFINE_bool('max_pool', False, 'Whether or not to use max pooling rather than strided convolutions')
flags.DEFINE_bool('stop_grad', False, 'if True, do not use second derivatives in meta-optimization (for speed)')

## Logging, saving, and testing options
flags.DEFINE_bool('log', True, 'if false, do not log summaries, for debugging code.')
flags.DEFINE_string('logdir', 'logs/fun', 'directory for summaries and checkpoints.')
flags.DEFINE_bool('resume', False, 'resume training if there is a model available')
flags.DEFINE_bool('train', True, 'True to train, False to test.')
flags.DEFINE_integer('test_iter', -1, 'iteration to load model (-1 for latest model)')
flags.DEFINE_bool('test_set', False, 'Set to True to test on the the test set, False for the validation set.')
flags.DEFINE_integer('train_update_batch_size', -1, 'number of examples used for gradient update during training (use if you want to test with a different number).')
flags.DEFINE_float('train_update_lr', -1, 'value of inner gradient step during training. (use if you want to test with a different value)') # 0.1 for omniglot

def train(model, saver, sess, exp_string, data_generator, resume_itr=0):
    SUMMARY_INTERVAL = 100
    SAVE_INTERVAL = 1000
    if FLAGS.datasource == 'sinusoid':
        PRINT_INTERVAL = 1000
        TEST_PRINT_INTERVAL = PRINT_INTERVAL*5
    elif FLAGS.datasource == 'fun':
        PRINT_INTERVAL = 1000
        TEST_PRINT_INTERVAL = PRINT_INTERVAL*2
    else:
        PRINT_INTERVAL = 100
        TEST_PRINT_INTERVAL = PRINT_INTERVAL*5
    if FLAGS.log:
        train_writer = tf.summary.FileWriter(FLAGS.logdir + '/' + exp_string, sess.graph)
    print('Done initializing, starting training.')
    prelosses, postlosses = [], []
    
    multitask_weights, reg_weights = [], []  # not used anywhere ??

    for itr in range(resume_itr, FLAGS.pretrain_iterations + FLAGS.metatrain_iterations):
        feed_dict = {}
        if 'generate' in dir(data_generator):
            if FLAGS.datasource == 'fun':
                res = data_generator.generate()
                x_train = res["x_train"]
                x_val = res["x_val"]
                x_test = res["x_test"]
                y_train = res["y_train"]
                y_val = res["y_val"]
                y_test = res["y_test"]
            elif FLAGS.datasource == 'sinusoid':
            	# batch_x, batch_y shape:
                #
                #     batch_x = [fun_i, x_i, 1]
                #     batch_y = [fun_i, f(x_i), 1]
                #
                # where fun_i identifies a specific functional form, x_i the x-coordinates on which f(x_i) is evaluated.
                batch_x, batch_y, amp, phase = data_generator.generate(train=False)
            else:
                raise ValueError('i don\'t know this datasource...')
                
            if FLAGS.baseline == 'oracle' and FLAGS.datasource == 'sinusoid':
                batch_x = np.concatenate([batch_x, np.zeros([batch_x.shape[0], batch_x.shape[1], 2])], 2)
                for i in range(FLAGS.meta_batch_size):
                    batch_x[i, :, 1] = amp[i]
                    batch_x[i, :, 2] = phase[i]

            if FLAGS.datasource == 'sinusoid':
            	# ---- maml data generator arrangement ----------------------------------------------------------------
                # A batch is splitted into two halves, inputa and inputb. These two parts inputa, inputb come from the same function.
                # They are evaluated at different x_i. The x values are not sorted, so those in inputa might be greater than those in inputb.
                # The goal is to make the submodels M({x}_i, theta_i) learn and test on data from the same function.
                inputa = batch_x[:, :FLAGS.update_batch_size, :] # a is for few-shot training
                labela = batch_y[:, :FLAGS.update_batch_size, :]
                inputb = batch_x[:, FLAGS.update_batch_size:, :] # b is for validation
                labelb = batch_y[:, FLAGS.update_batch_size:, :]
            elif FLAGS.datasource == 'fun':
            	# ---- FunGenerator data arrangement ----------------------------------
            	# The generator returns a train and a validation set. No need for the complicated split.
                inputa = x_train # a is for few-shot training
                labela = y_train
                inputb = x_val # b is for validation
                labelb = y_val

            feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb}

        if itr < FLAGS.pretrain_iterations:
            input_tensors = [model.pretrain_op]
        else:
            # training op ======================
            # NOTE :
            # metatrain.op is a variable, which equals 
            #
            #     metatrain_op = optimizer.minimize(loss)
            #
            # By passing this into sess.run() below, it's doing one grad update  
            input_tensors = [model.metatrain_op]
            # ==================================

        if (itr % SUMMARY_INTERVAL == 0 or itr % PRINT_INTERVAL == 0):
            input_tensors.extend([model.summ_op, model.total_loss1, model.total_losses2[FLAGS.num_updates-1]])

        # one grad update =============================
        # NOTE :
        # This is just sess.run(metatrain_op) 
        result = sess.run(input_tensors, feed_dict)
        # =============================================
        #
        # Q : what does sess.run(metatrain_op) return? Not clear why I got these stuffs.
        # print('result len :', len(result))
        # print('result[-1] : ', result[-1])
        # print('result[-2] : ', result[-2])
        # print('result[0] : ', result[0])
        # print('result[1] : ', result[1])
        # res = sess.run(model.result, feed_dict)
        # print('res len :', len(res))
        # print('mean(res[2]) :', np.mean(res[2]))
        # res3 = res[3]
        # tmp = [np.sum(res3[j]) / FLAGS.meta_batch_size for j in range(FLAGS.num_updates)]
        # print('tmp :', tmp)
        # sys.exit()

        if itr % SUMMARY_INTERVAL == 0:
            prelosses.append(result[-2]) # result[-2]  is task_lossa
            if FLAGS.log:
                train_writer.add_summary(result[1], itr)
            postlosses.append(result[-1]) # result[-1] is task_lossesb, a list

        if (itr!=0) and itr % PRINT_INTERVAL == 0:
            if itr < FLAGS.pretrain_iterations:
                print_str = 'Pretrain Iteration ' + str(itr)
            else:
                print_str = 'Iter' + str(itr - FLAGS.pretrain_iterations)
            print_str += ' preslosses : ' + str(np.mean(prelosses)) + ', postlosses :' + str(np.mean(postlosses))
            print(print_str)
            prelosses, postlosses = [], []

        if (itr!=0) and itr % SAVE_INTERVAL == 0:
            saver.save(sess, FLAGS.logdir + '/' + exp_string + '/model' + str(itr))

        # sinusoid is infinite data, so no need to test on meta-validation set. [????]
        if (itr!=0) and (itr % TEST_PRINT_INTERVAL) == 0:
            if 'generate' not in dir(data_generator):
                feed_dict = {}
                input_tensors = [model.metaval_total_loss1, model.metaval_total_losses2[FLAGS.num_updates-1], model.summ_op]
            else:
                if FLAGS.datasource == 'sinusoid':
                    batch_x, batch_y, amp, phase= data_generator.generate(train=False)
                    inputa = batch_x[:, :FLAGS.update_batch_size, :]
                    inputb = batch_x[:, FLAGS.update_batch_size:, :]
                    labela = batch_y[:, :FLAGS.update_batch_size, :]
                    labelb = batch_y[:, FLAGS.update_batch_size:, :]
                elif FLAGS.datasource == 'fun':
                    res = data_generator.generate()
                    x_train = res["x_train"]
                    x_val = res["x_val"]
                    x_test = res["x_test"]
                    y_train = res["y_train"]
                    y_val = res["y_val"]
                    y_test = res["y_test"]
                    inputa = x_train # a is for few-shot training
                    labela = y_train
                    inputb = x_test # b is for validation
                    labelb = y_test
                else:
                    raise ValueError("i don't know this datasource.")

                feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb, model.meta_lr: 0.0}
                input_tensors = [model.total_loss1, model.total_losses2[FLAGS.num_updates-1]]

            result = sess.run(input_tensors, feed_dict)
            print('Test set total_loss1: ' + str(result[0]) + ', total_losses2[-1] : ' + str(result[1]))

    saver.save(sess, FLAGS.logdir + '/' + exp_string +  '/model' + str(itr))

# calculated for omniglot
NUM_TEST_POINTS = 5

def test(model, saver, sess, exp_string, data_generator, test_num_updates, gnm):
    print('update_batch_size :', FLAGS.update_batch_size)
    print('test_num_updates :', test_num_updates)
    print('exp_string :', exp_string)

    if FLAGS.datasource == 'sinusoid':
        # override the input
        data_generator = DataGenerator(FLAGS.update_batch_size*2, NUM_TEST_POINTS)

    #np.random.seed(17)
    #random.seed(17)
    num_classes = 1

    metaval_accuracies = []
    OutAs = []
    OutBs = []

    pp = PdfPages(gnm)
    fig, ax = plt.subplots(NUM_TEST_POINTS, 2, sharex=True, sharey=True, figsize=(14, 8.27))

    for ii in range(NUM_TEST_POINTS):
        if 'generate' not in dir(data_generator):
            feed_dict = {}
            feed_dict = {model.meta_lr : 0.0}
        else:
            if FLAGS.datasource == 'sinusoid':
                batch_x, batch_y, amp, phase = data_generator.generate(train=False, test_amp_rng=None, test_offset=None)
            elif FLAGS.datasource == 'fun':
                batch_x, batch_y, amp, phase = data_generator.generate(num_pts=1000, randomize=True, batch_size=5, x_range=[0.0,3.0], train=False)
            else:
                batch_x, batch_y, amp, phase = data_generator.generate(train=False)
            print('batch_x shape:', batch_x.shape)

            if FLAGS.baseline == 'oracle': # NOTE - this flag is specific to sinusoid
                batch_x = np.concatenate([batch_x, np.zeros([batch_x.shape[0], batch_x.shape[1], 2])], 2)
                batch_x[0, :, 1] = amp[0]
                batch_x[0, :, 2] = phase[0]

            # inputa = batch_x[:, :num_classes*FLAGS.update_batch_size, :]
            # labela = batch_y[:, :num_classes*FLAGS.update_batch_size, :]
            # inputb = batch_x[:, num_classes*FLAGS.update_batch_size:, :]
            # labelb = batch_y[:, num_classes*FLAGS.update_batch_size:, :]
            if FLAGS.datasource == 'fun':
                inputa = batch_x[:, :500, :]
                labela = batch_y[:, :500, :]
                inputb = batch_x[:, 500:, :]
                labelb = batch_y[:, 500:, :]
            else:
                inputa = batch_x[:, :FLAGS.update_batch_size, :]
                labela = batch_y[:, :FLAGS.update_batch_size, :]
                inputb = batch_x[:, FLAGS.update_batch_size:, :]
                labelb = batch_y[:, FLAGS.update_batch_size:, :]

            # why feed a meta_lr, which is 0.0 ?
            feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb, model.meta_lr: 0.0}

        if model.classification:
            result = sess.run([model.metaval_total_accuracy1] + model.metaval_total_accuracies2, feed_dict)
            metaval_accuracies.append(result)
        else:  # this is for sinusoid and Fun 
            # python : [value] + some_list = a list with value added to the top of the list
            tot_loss = sess.run([model.total_loss1] + model.total_losses2, feed_dict)
            #tot1 = sess.run(model.total_loss1, feed_dict)
            #tot2 = sess.run(model.total_losses2, feed_dict)
            #tot_loss = [tot1] + tot2
            #print('tot1 :', tot1)
            #print('tot2 :', tot2)
            #print('tot_loss :', tot_loss)

            # [HT] Plot the actual vs predict :
            # outAs, outBs, lossA, lossB = sess.run(model.result, feed_dict)
            # print('lossA len', len(lossA))
            # print('lossB len', len(lossB))
            #l1 = np.sum(lossA) / 1.0 # one batch
            #l2 = [np.sum(lossB[j]) / 1.0 for j in range(10)] 
            #tot_loss_calc = [l1] + l2
            #delta = np.array(tot_loss) - np.array(tot_loss_calc)
            #print('delta :', delta) # they are identical.
            # outAs = np.array(outAs)
            # outBs = np.array(outBs)
            # print('outAs shape :', outAs.shape)
            # print('outBs shape :', outBs.shape)
            # print('outAs[0,:5,0]:', outAs[0,:5,0])
            # print('outBs[0,0,:5,0]:', outBs[0,0,:5,0])
            # print('outBs[1,0,:5,0]:', outBs[1,0,:5,0])
            # print('outBs[3,0,:5,0]:', outBs[3,0,:5,0])
            # print('outBs[4,0,:5,0]:', outBs[4,0,:5,0])

            # CHECK against what i got from model.result 
            outAs1, outBs1  = sess.run([model.outputas, model.outputbs], feed_dict)      
            outAs1 = np.array(outAs1)
            outBs1 = np.array(outBs1)
            # print('outAs1 shape :', outAs1.shape)
            # print('outBs1 shape :', outBs1.shape)
            # print('outAs1[0,:5,0]:', outAs1[0,:5,0])
            # print('outBs1[0,0,:5,0]:', outBs1[0,0,:5,0])
            # print('outBs1[1,0,:5,0]:', outBs1[1,0,:5,0])
            # print('outBs1[3,0,:5,0]:', outBs1[3,0,:5,0])
            # print('outBs1[4,0,:5,0]:', outBs1[4,0,:5,0])      
            # Exactly the same as what come out of model.result. Reassuring !
            
            # Fit on training data (inputa)
            ax[ii,0].plot(inputa[ii,:,0], labela[ii,:,0], 'o', markersize=4, color='blue', label="input(A)")
            ax[ii,0].plot(inputa[ii,:,0], outAs1[ii,:,0], 'o', markersize=4, color='red', label="output(A)")
            ax[ii,0].grid(True)
            if ii == 0:
                ax[ii,0].set_title('Meta-learned Init Predict vs Actual')
                ax[ii,0].legend(loc="upper right")
            # Predict on test data (inputb)
            ax[ii,1].plot(inputb[ii,:,0], labelb[ii,:,0], 'o', markersize=4, color='blue', label="input(B)")            
            ax[ii,1].plot(inputb[ii,:,0], outBs1[1,ii,:,0], 's', markersize=4, color='green', label="predict, one-step")
            ax[ii,1].plot(inputb[ii,:,0], outBs1[test_num_updates-1,ii,:,0],'o', markersize=4, color='red', label="predict, %d-steps" % test_num_updates)  
            ax[ii,1].grid(True)          
            if ii == 0:
                ax[ii,1].set_title('Predict aft N-step update vs Actual')
                ax[ii,1].legend(loc="lower left")
                ax[ii,1].text(0.1, 0.7, 'test_num_updates:%d' % test_num_updates)
            
            metaval_accuracies.append(tot_loss)
            OutAs.append(outAs1)
            OutBs.append(outBs1)
        
    # if model.classification is False:
    #     plt.show() 

    metaval_accuracies = np.array(metaval_accuracies)
    means = np.mean(metaval_accuracies, 0)
    stds = np.std(metaval_accuracies, 0)
    ci95 = 1.96*stds/np.sqrt(NUM_TEST_POINTS)

    print('Mean validation accuracy/loss, stddev, and confidence intervals')
    print('means :\n', means)
    print('std :\n', stds)
    print('ci95 :\n', ci95)

    plt.tight_layout()
    pp.savefig(fig, orientation = 'landscape')
    pp.close()

    out_filename = FLAGS.logdir +'/'+ exp_string + '/' + 'test_ubs' + str(FLAGS.update_batch_size) + '_stepsize' + str(FLAGS.update_lr) + '.csv'
    out_pkl = FLAGS.logdir +'/'+ exp_string + '/' + 'test_ubs' + str(FLAGS.update_batch_size) + '_stepsize' + str(FLAGS.update_lr) + '.pkl'
    with open(out_pkl, 'wb') as f:
        pickle.dump({'mses': metaval_accuracies}, f)
    with open(out_filename, 'w') as f:
        writer = csv.writer(f, delimiter=',')
        writer.writerow(['update'+str(i) for i in range(len(means))])
        writer.writerow(means)
        writer.writerow(stds)
        writer.writerow(ci95)

    # param_fnm = 'W_and_b.pkl'    
    # with open(param_fnm, 'wb') as f:
    #     pickle.dump({'weights': model.weights}, f)
    #     # pickle.dump({'w2': W2}, f)
    #                     # pickle.dump({'w3': W3}, f)
    #                     # pickle.dump({'b1': B1}, f)
    #                     # pickle.dump({'b2': B2}, f)
    #                     # pickle.dump({'b3': B3}, f)
    # f.close()   
    

def main():

    n_sd = 0.01
    amp_rng = [1.0, 1.0]
    x_rng = [0, 2*np.pi]
    w = [0.25, 0.25] # smaller w ==> smoother
    ph = [0, 4*np.pi]
    ff = [2, 2]
    param_rng = {'x':x_rng, 'amp':amp_rng, 'freq':w, 'phase':ph, 'function':ff}
    # train, validation, test split
    split = [0.4, 0.4, 0.2]
   
    if FLAGS.train:
        test_num_updates = 10
    else:
        test_num_updates = 10
    
    if FLAGS.train == False:
        orig_meta_batch_size = FLAGS.meta_batch_size
        # always use meta batch size of 1 when testing.
        FLAGS.meta_batch_size = 1

    if FLAGS.datasource == 'sinusoid':
        # Need a factor of 2 in num_samples_per_fun because half is for training, the other
        # half for testing. So first half is inputa, 2nd half inputb
        data_generator = DataGenerator(FLAGS.update_batch_size*2, FLAGS.meta_batch_size)
    elif FLAGS.datasource == 'fun':
        data_generator = FunGenerator(num_pts=FLAGS.update_batch_size, batch_size=FLAGS.meta_batch_size, \
        	      param_range=param_rng, noise_sd=n_sd, train_test_split=split, dim_input=1, dim_output=1)
    else:
        print('datasource must be either sinusoid or fun.')
        sys.exit()        

    dim_output = data_generator.dim_output
    if FLAGS.baseline == 'oracle':
        assert FLAGS.datasource == 'sinusoid'
        dim_input = 3
        FLAGS.pretrain_iterations += FLAGS.metatrain_iterations
        FLAGS.metatrain_iterations = 0
    else:
        dim_input = data_generator.dim_input

    tf_data_load = False
    input_tensors = None

    # STEP 1. Create model =======================================================
    model = MAML(dim_input, dim_output, test_num_updates=test_num_updates)
    # ============================================================================

    # STEP 2. Contruct the model, define task_metalearn method in this call 
    if FLAGS.train or not tf_data_load:
        # build graph =========================================================
        model.construct_model(input_tensors=input_tensors, prefix='metatrain_')
        # =====================================================================
    if tf_data_load:
        model.construct_model(input_tensors=metaval_input_tensors, prefix='metaval_')
    model.summ_op = tf.summary.merge_all()

    saver = loader = tf.train.Saver(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES), max_to_keep=10)

    sess = tf.InteractiveSession()

    if FLAGS.train == False:
        # change to original meta batch size when loading model.
        FLAGS.meta_batch_size = orig_meta_batch_size

    if FLAGS.train_update_batch_size == -1:
        FLAGS.train_update_batch_size = FLAGS.update_batch_size
    if FLAGS.train_update_lr == -1:
        FLAGS.train_update_lr = FLAGS.update_lr

    exp_string = 'mbs_'+str(FLAGS.meta_batch_size) + '.ubs_' + str(FLAGS.train_update_batch_size) + '.numstep' + str(FLAGS.num_updates) + '.updatelr' + str(FLAGS.train_update_lr)

    if FLAGS.stop_grad:
        exp_string += 'stopgrad'
    if FLAGS.baseline:
        exp_string += FLAGS.baseline

    if FLAGS.norm == 'batch_norm':
        exp_string += 'batchnorm'
    elif FLAGS.norm == 'layer_norm':
        exp_string += 'layernorm'
    elif FLAGS.norm == 'None':
        exp_string += 'nonorm'
    else:
        raise ValueError('Norm setting not recognized.')

    resume_itr = 0
    model_file = None

    tf.global_variables_initializer().run()
    tf.train.start_queue_runners()

    if FLAGS.resume or not FLAGS.train:
        model_file = tf.train.latest_checkpoint(FLAGS.logdir + '/' + exp_string)
        print('model_file :', model_file)
        if FLAGS.test_iter > 0:
            model_file = model_file[:model_file.index('model')] + 'model' + str(FLAGS.test_iter)
        if model_file:
            ind1 = model_file.index('model')
            print('index(\"model\") :', ind1)
            resume_itr = int(model_file[ind1+5:])
            print('resume_itr :', resume_itr)
            print('bef restore, does var weights exist?', )
            if 'weights' in globals():
                print('yes')
            else:
                print('noop')
            print("Restoring model weights from " + model_file)
            # ===============================
            saver.restore(sess, model_file)
            # ===============================
            # Write weights parameters to file for use in FitPerFun.py
            with tf.variable_scope('model', reuse=None) as old_scope:
                print('dir(model):\n', dir(model))
                if 'weights' in dir(model):
                    # if 'weights' exists in the 'model' scope, then re-use it
                    old_scope.reuse_variables()
                    # assign it to python name 'weights'
                    weights = model.weights
                    print('weights.keys :\n', weights.keys())
                    print('W1 shape:', weights['w1'].shape)
                    print('W2 shape:', weights['w2'].shape)
                    print('W3 shape:', weights['w3'].shape)
                    print('B1 shape:', weights['b1'].shape)
                    print('B2 shape:', weights['b2'].shape)
                    print('B3 shape:', weights['b3'].shape)
                else:
                    print('i don`t see any weights in model!')

    if FLAGS.train:
        print('.... call train()')
        train(model, saver, sess, exp_string, data_generator, resume_itr)
    else:
        print('.... call test()')
        gnm = model_file + '_Fit_Predict_Plots' + '.pdf'
        test(model, saver, sess, exp_string, data_generator, test_num_updates, gnm)
        
        
if __name__ == "__main__":
    main()
